//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31294372
// Cuda compilation tools, release 11.7, V11.7.64
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_80
.address_size 64

	// .globl	feed_forward_fc1_m4096_n512_k128
// _ZZ32feed_forward_fc1_m4096_n512_k128E14compute_shared has been demoted
// _ZZ32feed_forward_fc1_m4096_n512_k128E13weight_shared has been demoted

.visible .entry feed_forward_fc1_m4096_n512_k128(
	.param .u64 feed_forward_fc1_m4096_n512_k128_param_0,
	.param .u64 feed_forward_fc1_m4096_n512_k128_param_1,
	.param .u64 feed_forward_fc1_m4096_n512_k128_param_2,
	.param .u64 feed_forward_fc1_m4096_n512_k128_param_3,
	.param .u64 feed_forward_fc1_m4096_n512_k128_param_4
)
.maxntid 256, 1, 1
{
	.reg .pred 	%p<119>;
	.reg .b16 	%rs<93>;
	.reg .f32 	%f<675>;
	.reg .b32 	%r<402>;
	.reg .b64 	%rd<46>;
	// demoted variable
	.shared .align 2 .b8 _ZZ32feed_forward_fc1_m4096_n512_k128E14compute_shared[4608];
	// demoted variable
	.shared .align 2 .b8 _ZZ32feed_forward_fc1_m4096_n512_k128E13weight_shared[18432];

	ld.param.u64 	%rd13, [feed_forward_fc1_m4096_n512_k128_param_0];
	ld.param.u64 	%rd14, [feed_forward_fc1_m4096_n512_k128_param_1];
	ld.param.u64 	%rd15, [feed_forward_fc1_m4096_n512_k128_param_2];
	ld.param.u64 	%rd16, [feed_forward_fc1_m4096_n512_k128_param_3];
	cvta.to.global.u64 	%rd17, %rd15;
	mov.f32 	%f94, 0f00000000;
	mov.u32 	%r399, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f94;}

	// end inline asm
	mov.b32 	%r391, {%rs10, %rs10};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f94;}

	// end inline asm
	mov.b32 	%r395, {%rs11, %rs11};
	mov.u32 	%r34, %tid.x;
	shr.s32 	%r35, %r34, 4;
	shl.b32 	%r36, %r34, 2;
	and.b32  	%r37, %r36, 60;
	mov.u32 	%r38, %ctaid.x;
	shl.b32 	%r39, %r38, 5;
	mov.u32 	%r40, %tid.z;
	shl.b32 	%r41, %r40, 2;
	mov.u32 	%r42, %tid.y;
	shl.b32 	%r43, %r42, 1;
	add.s32 	%r44, %r41, %r39;
	add.s32 	%r45, %r44, %r43;
	add.s32 	%r46, %r45, %r35;
	mul.lo.s32 	%r47, %r40, 288;
	mad.lo.s32 	%r48, %r42, 144, %r47;
	mad.lo.s32 	%r49, %r35, 72, %r48;
	add.s32 	%r50, %r49, %r37;
	cvta.to.global.u64 	%rd18, %rd14;
	mul.wide.s32 	%rd19, %r46, 2;
	add.s64 	%rd1, %rd18, %rd19;
	ld.global.nc.u16 	%rs1, [%rd1];
	add.s64 	%rd2, %rd17, %rd19;
	shl.b32 	%r51, %r50, 1;
	mov.u32 	%r52, _ZZ32feed_forward_fc1_m4096_n512_k128E14compute_shared;
	add.s32 	%r3, %r52, %r51;
	mov.u32 	%r53, _ZZ32feed_forward_fc1_m4096_n512_k128E13weight_shared;
	add.s32 	%r4, %r53, %r51;
	mad.lo.s32 	%r54, %r42, 2304, %r52;
	mad.lo.s32 	%r55, %r40, 4608, %r53;
	add.s32 	%r5, %r55, 32;
	add.s32 	%r6, %r55, 2336;
	add.s32 	%r7, %r54, 64;
	shl.b32 	%r56, %r38, 12;
	or.b32  	%r57, %r37, %r56;
	shl.b32 	%r58, %r40, 9;
	add.s32 	%r59, %r57, %r58;
	shl.b32 	%r60, %r42, 8;
	add.s32 	%r61, %r59, %r60;
	shl.b32 	%r62, %r35, 7;
	add.s32 	%r63, %r61, %r62;
	cvta.to.global.u64 	%rd20, %rd13;
	mul.wide.s32 	%rd21, %r63, 2;
	add.s64 	%rd22, %rd20, %rd21;
	add.s64 	%rd44, %rd22, 4096;
	mov.u32 	%r64, %ctaid.y;
	shl.b32 	%r65, %r64, 14;
	or.b32  	%r66, %r37, %r65;
	add.s32 	%r67, %r66, %r58;
	add.s32 	%r68, %r67, %r60;
	add.s32 	%r69, %r68, %r62;
	cvta.to.global.u64 	%rd23, %rd16;
	mul.wide.s32 	%rd24, %r69, 2;
	add.s64 	%rd25, %rd23, %rd24;
	add.s64 	%rd43, %rd25, 16384;
	mov.u32 	%r392, %r391;
	mov.u32 	%r393, %r391;
	mov.u32 	%r394, %r391;
	mov.u32 	%r396, %r395;
	mov.u32 	%r397, %r395;
	mov.u32 	%r398, %r395;

$L__BB0_1:
	add.s32 	%r73, %r55, 2400;
	add.s32 	%r74, %r55, 96;
	add.s32 	%r78, %r54, 96;
	add.s32 	%r79, %r55, 2368;
	add.s32 	%r80, %r55, 64;
	add.s32 	%r81, %r54, 32;
	add.s32 	%r82, %r55, 2304;
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r83, %r84}, [%rd44+-4096];
	mov.b32 	{%rs13, %rs16}, %r83;
	// begin inline asm
	{sub.f16 %rs12,%rs13,%rs1;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs15,%rs16,%rs1;
}
	// end inline asm
	mov.b32 	{%rs19, %rs22}, %r84;
	// begin inline asm
	{sub.f16 %rs18,%rs19,%rs1;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs21,%rs22,%rs1;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f95, %rs12;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f96, %rs15;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f97, %rs18;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f98, %rs21;}

	// end inline asm
	ld.global.nc.u16 	%rs28, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f99, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f100, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f101, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f102, %rs28;}

	// end inline asm
	fma.rn.f32 	%f119, %f99, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f120, %f100, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f121, %f101, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f122, %f102, 0f3C000000, 0f47C35000;
	div.rn.f32 	%f103, %f95, %f119;
	div.rn.f32 	%f104, %f96, %f120;
	div.rn.f32 	%f105, %f97, %f121;
	div.rn.f32 	%f106, %f98, %f122;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f104;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f103;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs35, %f106;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f105;}

	// end inline asm
	st.shared.v2.u16 	[%r3], {%rs32, %rs33};
	st.shared.v2.u16 	[%r3+4], {%rs34, %rs35};
	ld.global.nc.v2.u32 	{%r87, %r88}, [%rd44];
	ld.global.nc.u16 	%rs38, [%rd1+32];
	mov.b32 	{%rs37, %rs40}, %r87;
	// begin inline asm
	{sub.f16 %rs36,%rs37,%rs38;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs39,%rs40,%rs38;
}
	// end inline asm
	mov.b32 	{%rs43, %rs46}, %r88;
	// begin inline asm
	{sub.f16 %rs42,%rs43,%rs38;
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs45,%rs46,%rs38;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f107, %rs36;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f108, %rs39;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f109, %rs42;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f110, %rs45;}

	// end inline asm
	ld.global.nc.u16 	%rs52, [%rd2+32];
	// begin inline asm
	{  cvt.f32.f16 %f111, %rs52;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f112, %rs52;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f113, %rs52;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f114, %rs52;}

	// end inline asm
	fma.rn.f32 	%f123, %f111, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f124, %f112, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f125, %f113, 0f3C000000, 0f47C35000;
	fma.rn.f32 	%f126, %f114, 0f3C000000, 0f47C35000;
	div.rn.f32 	%f115, %f107, %f123;
	div.rn.f32 	%f116, %f108, %f124;
	div.rn.f32 	%f117, %f109, %f125;
	div.rn.f32 	%f118, %f110, %f126;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f118;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs58, %f117;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs57, %f116;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs56, %f115;}

	// end inline asm
	st.shared.v4.u16 	[%r3+2304], {%rs56, %rs57, %rs58, %rs59};
	ld.global.nc.v2.u32 	{%r91, %r92}, [%rd43+-16384];
	st.shared.v2.u32 	[%r4], {%r91, %r92};
	ld.global.nc.v2.u32 	{%r95, %r96}, [%rd43+-12288];
	st.shared.v2.u32 	[%r4+2304], {%r95, %r96};
	ld.global.nc.v2.u32 	{%r99, %r100}, [%rd43+-8192];
	st.shared.v2.u32 	[%r4+4608], {%r99, %r100};
	ld.global.nc.v2.u32 	{%r103, %r104}, [%rd43+-4096];
	st.shared.v2.u32 	[%r4+6912], {%r103, %r104};
	ld.global.nc.v2.u32 	{%r107, %r108}, [%rd43];
	st.shared.v2.u32 	[%r4+9216], {%r107, %r108};
	ld.global.nc.v2.u32 	{%r111, %r112}, [%rd43+4096];
	st.shared.v2.u32 	[%r4+11520], {%r111, %r112};
	ld.global.nc.v2.u32 	{%r115, %r116}, [%rd43+8192];
	st.shared.v2.u32 	[%r4+13824], {%r115, %r116};
	ld.global.nc.v2.u32 	{%r119, %r120}, [%rd43+12288];
	st.shared.v2.u32 	[%r4+16128], {%r119, %r120};
	bar.sync 	0;
	mov.u32 	%r123, 72;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r124, %r125, %r126, %r127, %r128, %r129, %r130, %r131}, [%r54], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r132, %r133, %r134, %r135, %r136, %r137, %r138, %r139}, [%r55], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r140, %r141, %r142, %r143, %r144, %r145, %r146, %r147}, [%r82], %r123;
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r148, %r149, %r150, %r151}, {%r124, %r125, %r126, %r127, %r128, %r129, %r130, %r131}, {%r132, %r133, %r134, %r135, %r136, %r137, %r138, %r139}, {%r391, %r392, %r393, %r394};
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r152, %r153, %r154, %r155}, {%r124, %r125, %r126, %r127, %r128, %r129, %r130, %r131}, {%r140, %r141, %r142, %r143, %r144, %r145, %r146, %r147}, {%r395, %r396, %r397, %r398};
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r156, %r157, %r158, %r159, %r160, %r161, %r162, %r163}, [%r81], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r164, %r165, %r166, %r167, %r168, %r169, %r170, %r171}, [%r5], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r172, %r173, %r174, %r175, %r176, %r177, %r178, %r179}, [%r6], %r123;
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r180, %r181, %r182, %r183}, {%r156, %r157, %r158, %r159, %r160, %r161, %r162, %r163}, {%r164, %r165, %r166, %r167, %r168, %r169, %r170, %r171}, {%r148, %r149, %r150, %r151};
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r184, %r185, %r186, %r187}, {%r156, %r157, %r158, %r159, %r160, %r161, %r162, %r163}, {%r172, %r173, %r174, %r175, %r176, %r177, %r178, %r179}, {%r152, %r153, %r154, %r155};
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r188, %r189, %r190, %r191, %r192, %r193, %r194, %r195}, [%r7], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r196, %r197, %r198, %r199, %r200, %r201, %r202, %r203}, [%r80], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r204, %r205, %r206, %r207, %r208, %r209, %r210, %r211}, [%r79], %r123;
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r212, %r213, %r214, %r215}, {%r188, %r189, %r190, %r191, %r192, %r193, %r194, %r195}, {%r196, %r197, %r198, %r199, %r200, %r201, %r202, %r203}, {%r180, %r181, %r182, %r183};
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r216, %r217, %r218, %r219}, {%r188, %r189, %r190, %r191, %r192, %r193, %r194, %r195}, {%r204, %r205, %r206, %r207, %r208, %r209, %r210, %r211}, {%r184, %r185, %r186, %r187};
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r220, %r221, %r222, %r223, %r224, %r225, %r226, %r227}, [%r78], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r228, %r229, %r230, %r231, %r232, %r233, %r234, %r235}, [%r74], %r123;
	wmma.load.b.sync.aligned.col.m16n16k16.shared.f16 	{%r236, %r237, %r238, %r239, %r240, %r241, %r242, %r243}, [%r73], %r123;
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r391, %r392, %r393, %r394}, {%r220, %r221, %r222, %r223, %r224, %r225, %r226, %r227}, {%r228, %r229, %r230, %r231, %r232, %r233, %r234, %r235}, {%r212, %r213, %r214, %r215};
	wmma.mma.sync.aligned.row.col.m16n16k16.f16.f16 {%r395, %r396, %r397, %r398}, {%r220, %r221, %r222, %r223, %r224, %r225, %r226, %r227}, {%r236, %r237, %r238, %r239, %r240, %r241, %r242, %r243}, {%r216, %r217, %r218, %r219};
	add.s64 	%rd44, %rd44, 128;
	add.s64 	%rd43, %rd43, 128;
	add.s32 	%r399, %r399, 1;
	setp.lt.u32 	%p5, %r399, 2;
	@%p5 bra 	$L__BB0_1;

	ld.param.u64 	%rd42, [feed_forward_fc1_m4096_n512_k128_param_4];
	mov.u32 	%r390, %tid.x;
	shl.b32 	%r389, %r390, 2;
	mov.u32 	%r388, %ctaid.y;
	mov.u32 	%r387, %ctaid.x;
	mov.u32 	%r386, _ZZ32feed_forward_fc1_m4096_n512_k128E13weight_shared;
	mov.u32 	%r385, %tid.y;
	mov.u32 	%r384, %tid.z;
	bar.sync 	0;
	shl.b32 	%r247, %r384, 5;
	mad.lo.s32 	%r248, %r385, 2176, %r247;
	shl.b32 	%r249, %r248, 1;
	add.s32 	%r251, %r386, %r249;
	mov.u32 	%r401, 0;
	mov.u32 	%r252, 136;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f16 	[%r251], {%r391, %r392, %r393, %r394}, %r252;
	add.s32 	%r253, %r251, 32;
	wmma.store.d.sync.aligned.row.m16n16k16.shared.f16 	[%r253], {%r395, %r396, %r397, %r398}, %r252;
	bar.sync 	0;
	shl.b32 	%r255, %r387, 14;
	shl.b32 	%r256, %r384, 10;
	add.s32 	%r257, %r255, %r256;
	shl.b32 	%r258, %r385, 9;
	add.s32 	%r259, %r257, %r258;
	shl.b32 	%r261, %r388, 7;
	add.s32 	%r262, %r259, %r261;
	add.s32 	%r265, %r262, %r389;
	cvta.to.global.u64 	%rd40, %rd42;
	mul.wide.s32 	%rd41, %r265, 2;
	add.s64 	%rd45, %rd40, %rd41;
	mul.lo.s32 	%r266, %r385, 136;
	mad.lo.s32 	%r267, %r384, 272, %r266;
	add.s32 	%r268, %r267, %r389;
	shl.b32 	%r269, %r268, 1;
	add.s32 	%r400, %r386, %r269;

$L__BB0_3:
	ld.shared.v2.u32 	{%r270, %r271}, [%r400];
	mov.b32 	{%rs72, %rs65}, %r270;
	// begin inline asm
	{  cvt.f32.f16 %f127, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f128, %rs65;}

	// end inline asm
	mov.b32 	{%rs66, %rs67}, %r271;
	// begin inline asm
	{  cvt.f32.f16 %f129, %rs66;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f130, %rs67;}

	// end inline asm
	mul.f32 	%f1, %f127, 0f3F000000;
	mul.f32 	%f2, %f128, 0f3F000000;
	mul.f32 	%f3, %f129, 0f3F000000;
	mul.f32 	%f4, %f130, 0f3F000000;
	// begin inline asm
	{  cvt.f32.f16 %f131, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f132, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f133, %rs66;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f134, %rs67;}

	// end inline asm
	mov.f32 	%f138, 0f40400000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f138;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f138;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f138;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f138;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f139, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f140, %rs68;}

	// end inline asm
	abs.f32 	%f12, %f139;
	setp.lt.f32 	%p6, %f12, 0f00800000;
	mul.f32 	%f146, %f12, 0f4B800000;
	selp.f32 	%f147, %f146, %f12, %p6;
	selp.f32 	%f148, 0fC3170000, 0fC2FE0000, %p6;
	mov.b32 	%r272, %f147;
	and.b32  	%r273, %r272, 8388607;
	or.b32  	%r274, %r273, 1065353216;
	mov.b32 	%f149, %r274;
	shr.u32 	%r275, %r272, 23;
	cvt.rn.f32.u32 	%f150, %r275;
	add.f32 	%f151, %f148, %f150;
	setp.gt.f32 	%p7, %f149, 0f3FB504F3;
	mul.f32 	%f152, %f149, 0f3F000000;
	add.f32 	%f153, %f151, 0f3F800000;
	selp.f32 	%f154, %f153, %f151, %p7;
	selp.f32 	%f155, %f152, %f149, %p7;
	add.f32 	%f156, %f155, 0fBF800000;
	add.f32 	%f157, %f155, 0f3F800000;
	rcp.approx.ftz.f32 	%f158, %f157;
	add.f32 	%f159, %f156, %f156;
	mul.f32 	%f160, %f159, %f158;
	mul.f32 	%f161, %f160, %f160;
	mov.f32 	%f162, 0f3C4CAF63;
	mov.f32 	%f163, 0f3B18F0FE;
	fma.rn.f32 	%f164, %f163, %f161, %f162;
	mov.f32 	%f165, 0f3DAAAABD;
	fma.rn.f32 	%f166, %f164, %f161, %f165;
	mul.rn.f32 	%f167, %f166, %f161;
	mul.rn.f32 	%f168, %f167, %f160;
	sub.f32 	%f169, %f156, %f160;
	add.f32 	%f170, %f169, %f169;
	neg.f32 	%f171, %f160;
	fma.rn.f32 	%f172, %f171, %f156, %f170;
	mul.rn.f32 	%f173, %f158, %f172;
	add.f32 	%f174, %f168, %f160;
	sub.f32 	%f175, %f160, %f174;
	add.f32 	%f176, %f168, %f175;
	add.f32 	%f177, %f173, %f176;
	add.f32 	%f178, %f174, %f177;
	sub.f32 	%f179, %f174, %f178;
	add.f32 	%f180, %f177, %f179;
	mov.f32 	%f181, 0f3F317200;
	mul.rn.f32 	%f182, %f154, %f181;
	mov.f32 	%f183, 0f35BFBE8E;
	mul.rn.f32 	%f184, %f154, %f183;
	add.f32 	%f185, %f182, %f178;
	sub.f32 	%f186, %f182, %f185;
	add.f32 	%f187, %f178, %f186;
	add.f32 	%f188, %f180, %f187;
	add.f32 	%f189, %f184, %f188;
	add.f32 	%f190, %f185, %f189;
	sub.f32 	%f191, %f185, %f190;
	add.f32 	%f192, %f189, %f191;
	abs.f32 	%f13, %f140;
	setp.gt.f32 	%p8, %f13, 0f77F684DF;
	mul.f32 	%f193, %f140, 0f39000000;
	selp.f32 	%f194, %f193, %f140, %p8;
	mul.rn.f32 	%f195, %f194, %f190;
	neg.f32 	%f196, %f195;
	fma.rn.f32 	%f197, %f194, %f190, %f196;
	fma.rn.f32 	%f198, %f194, %f192, %f197;
	fma.rn.f32 	%f200, %f94, %f190, %f198;
	add.rn.f32 	%f201, %f195, %f200;
	neg.f32 	%f202, %f201;
	add.rn.f32 	%f203, %f195, %f202;
	add.rn.f32 	%f204, %f203, %f200;
	mov.b32 	%r276, %f201;
	setp.eq.s32 	%p9, %r276, 1118925336;
	add.s32 	%r277, %r276, -1;
	mov.b32 	%f205, %r277;
	add.f32 	%f206, %f204, 0f37000000;
	selp.f32 	%f14, %f206, %f204, %p9;
	selp.f32 	%f207, %f205, %f201, %p9;
	mov.f32 	%f208, 0f3FB8AA3B;
	mul.rn.f32 	%f209, %f207, %f208;
	cvt.rzi.f32.f32 	%f210, %f209;
	abs.f32 	%f211, %f210;
	setp.gt.f32 	%p10, %f211, 0f42FC0000;
	mov.b32 	%r278, %f210;
	and.b32  	%r279, %r278, -2147483648;
	or.b32  	%r280, %r279, 1123811328;
	mov.b32 	%f212, %r280;
	selp.f32 	%f213, %f212, %f210, %p10;
	mov.f32 	%f214, 0fBF317218;
	fma.rn.f32 	%f215, %f213, %f214, %f207;
	mov.f32 	%f216, 0f3102E308;
	fma.rn.f32 	%f217, %f213, %f216, %f215;
	mul.f32 	%f218, %f217, 0f3FB8AA3B;
	add.f32 	%f219, %f213, 0f4B40007F;
	mov.b32 	%r281, %f219;
	shl.b32 	%r282, %r281, 23;
	mov.b32 	%f220, %r282;
	ex2.approx.ftz.f32 	%f221, %f218;
	mul.f32 	%f15, %f221, %f220;
	setp.eq.f32 	%p11, %f15, 0f7F800000;
	mov.f32 	%f659, 0f7F800000;
	@%p11 bra 	$L__BB0_5;

	fma.rn.f32 	%f659, %f15, %f14, %f15;

$L__BB0_5:
	mul.f32 	%f591, %f140, 0f3F000000;
	cvt.rzi.f32.f32 	%f590, %f591;
	add.f32 	%f589, %f590, %f590;
	sub.f32 	%f588, %f140, %f589;
	abs.f32 	%f587, %f588;
	setp.lt.f32 	%p12, %f139, 0f00000000;
	setp.eq.f32 	%p13, %f587, 0f3F800000;
	and.pred  	%p1, %p12, %p13;
	setp.eq.f32 	%p14, %f139, 0f00000000;
	@%p14 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_6;

$L__BB0_9:
	add.f32 	%f225, %f139, %f139;
	mov.b32 	%r285, %f225;
	selp.b32 	%r286, %r285, 0, %p13;
	or.b32  	%r287, %r286, 2139095040;
	setp.lt.f32 	%p18, %f140, 0f00000000;
	selp.b32 	%r288, %r287, %r286, %p18;
	mov.b32 	%f661, %r288;
	bra.uni 	$L__BB0_10;

$L__BB0_6:
	mov.b32 	%r283, %f659;
	xor.b32  	%r284, %r283, -2147483648;
	mov.b32 	%f222, %r284;
	selp.f32 	%f661, %f222, %f659, %p1;
	setp.geu.f32 	%p15, %f139, 0f00000000;
	@%p15 bra 	$L__BB0_10;

	cvt.rzi.f32.f32 	%f223, %f140;
	setp.eq.f32 	%p16, %f223, %f140;
	@%p16 bra 	$L__BB0_10;

	mov.f32 	%f661, 0f7FFFFFFF;

$L__BB0_10:
	abs.f32 	%f593, %f140;
	abs.f32 	%f592, %f139;
	add.f32 	%f226, %f592, %f593;
	mov.b32 	%r289, %f226;
	setp.lt.s32 	%p19, %r289, 2139095040;
	@%p19 bra 	$L__BB0_17;

	abs.f32 	%f619, %f140;
	abs.f32 	%f618, %f139;
	setp.gtu.f32 	%p20, %f618, 0f7F800000;
	setp.gtu.f32 	%p21, %f619, 0f7F800000;
	or.pred  	%p22, %p20, %p21;
	@%p22 bra 	$L__BB0_16;
	bra.uni 	$L__BB0_12;

$L__BB0_16:
	add.f32 	%f661, %f139, %f140;
	bra.uni 	$L__BB0_17;

$L__BB0_12:
	abs.f32 	%f620, %f140;
	setp.eq.f32 	%p23, %f620, 0f7F800000;
	@%p23 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_13;

$L__BB0_15:
	abs.f32 	%f622, %f139;
	setp.gt.f32 	%p26, %f622, 0f3F800000;
	selp.b32 	%r293, 2139095040, 0, %p26;
	xor.b32  	%r294, %r293, 2139095040;
	setp.lt.f32 	%p27, %f140, 0f00000000;
	selp.b32 	%r295, %r294, %r293, %p27;
	mov.b32 	%f227, %r295;
	setp.eq.f32 	%p28, %f139, 0fBF800000;
	selp.f32 	%f661, 0f3F800000, %f227, %p28;
	bra.uni 	$L__BB0_17;

$L__BB0_13:
	abs.f32 	%f621, %f139;
	setp.neu.f32 	%p24, %f621, 0f7F800000;
	@%p24 bra 	$L__BB0_17;

	setp.ge.f32 	%p25, %f140, 0f00000000;
	selp.b32 	%r290, 2139095040, 0, %p25;
	or.b32  	%r291, %r290, -2147483648;
	selp.b32 	%r292, %r291, %r290, %p1;
	mov.b32 	%f661, %r292;

$L__BB0_17:
	mov.f32 	%f601, 0f3102E308;
	mov.f32 	%f600, 0fBF317218;
	mov.f32 	%f599, 0f3FB8AA3B;
	mov.f32 	%f598, 0f35BFBE8E;
	mov.f32 	%f597, 0f3F317200;
	mov.f32 	%f596, 0f3DAAAABD;
	mov.f32 	%f595, 0f3C4CAF63;
	mov.f32 	%f594, 0f3B18F0FE;
	setp.eq.f32 	%p29, %f140, 0f00000000;
	setp.eq.f32 	%p30, %f139, 0f3F800000;
	or.pred  	%p31, %p30, %p29;
	selp.f32 	%f228, 0f3F800000, %f661, %p31;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f228;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f229, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f230, %rs69;}

	// end inline asm
	abs.f32 	%f28, %f229;
	setp.lt.f32 	%p32, %f28, 0f00800000;
	mul.f32 	%f237, %f28, 0f4B800000;
	selp.f32 	%f238, %f237, %f28, %p32;
	selp.f32 	%f239, 0fC3170000, 0fC2FE0000, %p32;
	mov.b32 	%r296, %f238;
	and.b32  	%r297, %r296, 8388607;
	or.b32  	%r298, %r297, 1065353216;
	mov.b32 	%f240, %r298;
	shr.u32 	%r299, %r296, 23;
	cvt.rn.f32.u32 	%f241, %r299;
	add.f32 	%f242, %f239, %f241;
	setp.gt.f32 	%p33, %f240, 0f3FB504F3;
	mul.f32 	%f243, %f240, 0f3F000000;
	add.f32 	%f244, %f242, 0f3F800000;
	selp.f32 	%f245, %f244, %f242, %p33;
	selp.f32 	%f246, %f243, %f240, %p33;
	add.f32 	%f247, %f246, 0fBF800000;
	add.f32 	%f248, %f246, 0f3F800000;
	rcp.approx.ftz.f32 	%f249, %f248;
	add.f32 	%f250, %f247, %f247;
	mul.f32 	%f251, %f250, %f249;
	mul.f32 	%f252, %f251, %f251;
	fma.rn.f32 	%f255, %f594, %f252, %f595;
	fma.rn.f32 	%f257, %f255, %f252, %f596;
	mul.rn.f32 	%f258, %f257, %f252;
	mul.rn.f32 	%f259, %f258, %f251;
	sub.f32 	%f260, %f247, %f251;
	add.f32 	%f261, %f260, %f260;
	neg.f32 	%f262, %f251;
	fma.rn.f32 	%f263, %f262, %f247, %f261;
	mul.rn.f32 	%f264, %f249, %f263;
	add.f32 	%f265, %f259, %f251;
	sub.f32 	%f266, %f251, %f265;
	add.f32 	%f267, %f259, %f266;
	add.f32 	%f268, %f264, %f267;
	add.f32 	%f269, %f265, %f268;
	sub.f32 	%f270, %f265, %f269;
	add.f32 	%f271, %f268, %f270;
	mul.rn.f32 	%f273, %f245, %f597;
	mul.rn.f32 	%f275, %f245, %f598;
	add.f32 	%f276, %f273, %f269;
	sub.f32 	%f277, %f273, %f276;
	add.f32 	%f278, %f269, %f277;
	add.f32 	%f279, %f271, %f278;
	add.f32 	%f280, %f275, %f279;
	add.f32 	%f281, %f276, %f280;
	sub.f32 	%f282, %f276, %f281;
	add.f32 	%f283, %f280, %f282;
	abs.f32 	%f29, %f230;
	setp.gt.f32 	%p34, %f29, 0f77F684DF;
	mul.f32 	%f284, %f230, 0f39000000;
	selp.f32 	%f285, %f284, %f230, %p34;
	mul.rn.f32 	%f286, %f285, %f281;
	neg.f32 	%f287, %f286;
	fma.rn.f32 	%f288, %f285, %f281, %f287;
	fma.rn.f32 	%f289, %f285, %f283, %f288;
	fma.rn.f32 	%f290, %f94, %f281, %f289;
	add.rn.f32 	%f291, %f286, %f290;
	neg.f32 	%f292, %f291;
	add.rn.f32 	%f293, %f286, %f292;
	add.rn.f32 	%f294, %f293, %f290;
	mov.b32 	%r300, %f291;
	setp.eq.s32 	%p35, %r300, 1118925336;
	add.s32 	%r301, %r300, -1;
	mov.b32 	%f295, %r301;
	add.f32 	%f296, %f294, 0f37000000;
	selp.f32 	%f30, %f296, %f294, %p35;
	selp.f32 	%f297, %f295, %f291, %p35;
	mul.rn.f32 	%f299, %f297, %f599;
	cvt.rzi.f32.f32 	%f300, %f299;
	abs.f32 	%f301, %f300;
	setp.gt.f32 	%p36, %f301, 0f42FC0000;
	mov.b32 	%r302, %f300;
	and.b32  	%r303, %r302, -2147483648;
	or.b32  	%r304, %r303, 1123811328;
	mov.b32 	%f302, %r304;
	selp.f32 	%f303, %f302, %f300, %p36;
	fma.rn.f32 	%f305, %f303, %f600, %f297;
	fma.rn.f32 	%f307, %f303, %f601, %f305;
	mul.f32 	%f308, %f307, 0f3FB8AA3B;
	add.f32 	%f309, %f303, 0f4B40007F;
	mov.b32 	%r305, %f309;
	shl.b32 	%r306, %r305, 23;
	mov.b32 	%f310, %r306;
	ex2.approx.ftz.f32 	%f311, %f308;
	mul.f32 	%f31, %f311, %f310;
	setp.eq.f32 	%p37, %f31, 0f7F800000;
	mov.f32 	%f662, 0f7F800000;
	@%p37 bra 	$L__BB0_19;

	fma.rn.f32 	%f662, %f31, %f30, %f31;

$L__BB0_19:
	mul.f32 	%f627, %f230, 0f3F000000;
	cvt.rzi.f32.f32 	%f626, %f627;
	add.f32 	%f625, %f626, %f626;
	sub.f32 	%f624, %f230, %f625;
	abs.f32 	%f623, %f624;
	setp.lt.f32 	%p38, %f229, 0f00000000;
	setp.eq.f32 	%p39, %f623, 0f3F800000;
	and.pred  	%p2, %p38, %p39;
	setp.eq.f32 	%p40, %f229, 0f00000000;
	@%p40 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_20;

$L__BB0_23:
	add.f32 	%f315, %f229, %f229;
	mov.b32 	%r309, %f315;
	selp.b32 	%r310, %r309, 0, %p39;
	or.b32  	%r311, %r310, 2139095040;
	setp.lt.f32 	%p44, %f230, 0f00000000;
	selp.b32 	%r312, %r311, %r310, %p44;
	mov.b32 	%f664, %r312;
	bra.uni 	$L__BB0_24;

$L__BB0_20:
	mov.b32 	%r307, %f662;
	xor.b32  	%r308, %r307, -2147483648;
	mov.b32 	%f312, %r308;
	selp.f32 	%f664, %f312, %f662, %p2;
	setp.geu.f32 	%p41, %f229, 0f00000000;
	@%p41 bra 	$L__BB0_24;

	cvt.rzi.f32.f32 	%f313, %f230;
	setp.eq.f32 	%p42, %f313, %f230;
	@%p42 bra 	$L__BB0_24;

	mov.f32 	%f664, 0f7FFFFFFF;

$L__BB0_24:
	abs.f32 	%f629, %f230;
	abs.f32 	%f628, %f229;
	add.f32 	%f316, %f628, %f629;
	mov.b32 	%r313, %f316;
	setp.lt.s32 	%p45, %r313, 2139095040;
	@%p45 bra 	$L__BB0_31;

	abs.f32 	%f631, %f230;
	abs.f32 	%f630, %f229;
	setp.gtu.f32 	%p46, %f630, 0f7F800000;
	setp.gtu.f32 	%p47, %f631, 0f7F800000;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_26;

$L__BB0_30:
	add.f32 	%f664, %f229, %f230;
	bra.uni 	$L__BB0_31;

$L__BB0_26:
	abs.f32 	%f632, %f230;
	setp.eq.f32 	%p49, %f632, 0f7F800000;
	@%p49 bra 	$L__BB0_29;
	bra.uni 	$L__BB0_27;

$L__BB0_29:
	abs.f32 	%f634, %f229;
	setp.gt.f32 	%p52, %f634, 0f3F800000;
	selp.b32 	%r317, 2139095040, 0, %p52;
	xor.b32  	%r318, %r317, 2139095040;
	setp.lt.f32 	%p53, %f230, 0f00000000;
	selp.b32 	%r319, %r318, %r317, %p53;
	mov.b32 	%f317, %r319;
	setp.eq.f32 	%p54, %f229, 0fBF800000;
	selp.f32 	%f664, 0f3F800000, %f317, %p54;
	bra.uni 	$L__BB0_31;

$L__BB0_27:
	abs.f32 	%f633, %f229;
	setp.neu.f32 	%p50, %f633, 0f7F800000;
	@%p50 bra 	$L__BB0_31;

	setp.ge.f32 	%p51, %f230, 0f00000000;
	selp.b32 	%r314, 2139095040, 0, %p51;
	or.b32  	%r315, %r314, -2147483648;
	selp.b32 	%r316, %r315, %r314, %p2;
	mov.b32 	%f664, %r316;

$L__BB0_31:
	mov.f32 	%f609, 0f3102E308;
	mov.f32 	%f608, 0fBF317218;
	mov.f32 	%f607, 0f3FB8AA3B;
	mov.f32 	%f606, 0f35BFBE8E;
	mov.f32 	%f605, 0f3F317200;
	mov.f32 	%f604, 0f3DAAAABD;
	mov.f32 	%f603, 0f3C4CAF63;
	mov.f32 	%f602, 0f3B18F0FE;
	setp.eq.f32 	%p55, %f230, 0f00000000;
	setp.eq.f32 	%p56, %f229, 0f3F800000;
	or.pred  	%p57, %p56, %p55;
	selp.f32 	%f318, 0f3F800000, %f664, %p57;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f318;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f319, %rs66;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f320, %rs70;}

	// end inline asm
	abs.f32 	%f44, %f319;
	setp.lt.f32 	%p58, %f44, 0f00800000;
	mul.f32 	%f327, %f44, 0f4B800000;
	selp.f32 	%f328, %f327, %f44, %p58;
	selp.f32 	%f329, 0fC3170000, 0fC2FE0000, %p58;
	mov.b32 	%r320, %f328;
	and.b32  	%r321, %r320, 8388607;
	or.b32  	%r322, %r321, 1065353216;
	mov.b32 	%f330, %r322;
	shr.u32 	%r323, %r320, 23;
	cvt.rn.f32.u32 	%f331, %r323;
	add.f32 	%f332, %f329, %f331;
	setp.gt.f32 	%p59, %f330, 0f3FB504F3;
	mul.f32 	%f333, %f330, 0f3F000000;
	add.f32 	%f334, %f332, 0f3F800000;
	selp.f32 	%f335, %f334, %f332, %p59;
	selp.f32 	%f336, %f333, %f330, %p59;
	add.f32 	%f337, %f336, 0fBF800000;
	add.f32 	%f338, %f336, 0f3F800000;
	rcp.approx.ftz.f32 	%f339, %f338;
	add.f32 	%f340, %f337, %f337;
	mul.f32 	%f341, %f340, %f339;
	mul.f32 	%f342, %f341, %f341;
	fma.rn.f32 	%f345, %f602, %f342, %f603;
	fma.rn.f32 	%f347, %f345, %f342, %f604;
	mul.rn.f32 	%f348, %f347, %f342;
	mul.rn.f32 	%f349, %f348, %f341;
	sub.f32 	%f350, %f337, %f341;
	add.f32 	%f351, %f350, %f350;
	neg.f32 	%f352, %f341;
	fma.rn.f32 	%f353, %f352, %f337, %f351;
	mul.rn.f32 	%f354, %f339, %f353;
	add.f32 	%f355, %f349, %f341;
	sub.f32 	%f356, %f341, %f355;
	add.f32 	%f357, %f349, %f356;
	add.f32 	%f358, %f354, %f357;
	add.f32 	%f359, %f355, %f358;
	sub.f32 	%f360, %f355, %f359;
	add.f32 	%f361, %f358, %f360;
	mul.rn.f32 	%f363, %f335, %f605;
	mul.rn.f32 	%f365, %f335, %f606;
	add.f32 	%f366, %f363, %f359;
	sub.f32 	%f367, %f363, %f366;
	add.f32 	%f368, %f359, %f367;
	add.f32 	%f369, %f361, %f368;
	add.f32 	%f370, %f365, %f369;
	add.f32 	%f371, %f366, %f370;
	sub.f32 	%f372, %f366, %f371;
	add.f32 	%f373, %f370, %f372;
	abs.f32 	%f45, %f320;
	setp.gt.f32 	%p60, %f45, 0f77F684DF;
	mul.f32 	%f374, %f320, 0f39000000;
	selp.f32 	%f375, %f374, %f320, %p60;
	mul.rn.f32 	%f376, %f375, %f371;
	neg.f32 	%f377, %f376;
	fma.rn.f32 	%f378, %f375, %f371, %f377;
	fma.rn.f32 	%f379, %f375, %f373, %f378;
	fma.rn.f32 	%f380, %f94, %f371, %f379;
	add.rn.f32 	%f381, %f376, %f380;
	neg.f32 	%f382, %f381;
	add.rn.f32 	%f383, %f376, %f382;
	add.rn.f32 	%f384, %f383, %f380;
	mov.b32 	%r324, %f381;
	setp.eq.s32 	%p61, %r324, 1118925336;
	add.s32 	%r325, %r324, -1;
	mov.b32 	%f385, %r325;
	add.f32 	%f386, %f384, 0f37000000;
	selp.f32 	%f46, %f386, %f384, %p61;
	selp.f32 	%f387, %f385, %f381, %p61;
	mul.rn.f32 	%f389, %f387, %f607;
	cvt.rzi.f32.f32 	%f390, %f389;
	abs.f32 	%f391, %f390;
	setp.gt.f32 	%p62, %f391, 0f42FC0000;
	mov.b32 	%r326, %f390;
	and.b32  	%r327, %r326, -2147483648;
	or.b32  	%r328, %r327, 1123811328;
	mov.b32 	%f392, %r328;
	selp.f32 	%f393, %f392, %f390, %p62;
	fma.rn.f32 	%f395, %f393, %f608, %f387;
	fma.rn.f32 	%f397, %f393, %f609, %f395;
	mul.f32 	%f398, %f397, 0f3FB8AA3B;
	add.f32 	%f399, %f393, 0f4B40007F;
	mov.b32 	%r329, %f399;
	shl.b32 	%r330, %r329, 23;
	mov.b32 	%f400, %r330;
	ex2.approx.ftz.f32 	%f401, %f398;
	mul.f32 	%f47, %f401, %f400;
	setp.eq.f32 	%p63, %f47, 0f7F800000;
	mov.f32 	%f665, 0f7F800000;
	@%p63 bra 	$L__BB0_33;

	fma.rn.f32 	%f665, %f47, %f46, %f47;

$L__BB0_33:
	mul.f32 	%f639, %f320, 0f3F000000;
	cvt.rzi.f32.f32 	%f638, %f639;
	add.f32 	%f637, %f638, %f638;
	sub.f32 	%f636, %f320, %f637;
	abs.f32 	%f635, %f636;
	setp.lt.f32 	%p64, %f319, 0f00000000;
	setp.eq.f32 	%p65, %f635, 0f3F800000;
	and.pred  	%p3, %p64, %p65;
	setp.eq.f32 	%p66, %f319, 0f00000000;
	@%p66 bra 	$L__BB0_37;
	bra.uni 	$L__BB0_34;

$L__BB0_37:
	add.f32 	%f405, %f319, %f319;
	mov.b32 	%r333, %f405;
	selp.b32 	%r334, %r333, 0, %p65;
	or.b32  	%r335, %r334, 2139095040;
	setp.lt.f32 	%p70, %f320, 0f00000000;
	selp.b32 	%r336, %r335, %r334, %p70;
	mov.b32 	%f667, %r336;
	bra.uni 	$L__BB0_38;

$L__BB0_34:
	mov.b32 	%r331, %f665;
	xor.b32  	%r332, %r331, -2147483648;
	mov.b32 	%f402, %r332;
	selp.f32 	%f667, %f402, %f665, %p3;
	setp.geu.f32 	%p67, %f319, 0f00000000;
	@%p67 bra 	$L__BB0_38;

	cvt.rzi.f32.f32 	%f403, %f320;
	setp.eq.f32 	%p68, %f403, %f320;
	@%p68 bra 	$L__BB0_38;

	mov.f32 	%f667, 0f7FFFFFFF;

$L__BB0_38:
	abs.f32 	%f641, %f320;
	abs.f32 	%f640, %f319;
	add.f32 	%f406, %f640, %f641;
	mov.b32 	%r337, %f406;
	setp.lt.s32 	%p71, %r337, 2139095040;
	@%p71 bra 	$L__BB0_45;

	abs.f32 	%f643, %f320;
	abs.f32 	%f642, %f319;
	setp.gtu.f32 	%p72, %f642, 0f7F800000;
	setp.gtu.f32 	%p73, %f643, 0f7F800000;
	or.pred  	%p74, %p72, %p73;
	@%p74 bra 	$L__BB0_44;
	bra.uni 	$L__BB0_40;

$L__BB0_44:
	add.f32 	%f667, %f319, %f320;
	bra.uni 	$L__BB0_45;

$L__BB0_40:
	abs.f32 	%f644, %f320;
	setp.eq.f32 	%p75, %f644, 0f7F800000;
	@%p75 bra 	$L__BB0_43;
	bra.uni 	$L__BB0_41;

$L__BB0_43:
	abs.f32 	%f646, %f319;
	setp.gt.f32 	%p78, %f646, 0f3F800000;
	selp.b32 	%r341, 2139095040, 0, %p78;
	xor.b32  	%r342, %r341, 2139095040;
	setp.lt.f32 	%p79, %f320, 0f00000000;
	selp.b32 	%r343, %r342, %r341, %p79;
	mov.b32 	%f407, %r343;
	setp.eq.f32 	%p80, %f319, 0fBF800000;
	selp.f32 	%f667, 0f3F800000, %f407, %p80;
	bra.uni 	$L__BB0_45;

$L__BB0_41:
	abs.f32 	%f645, %f319;
	setp.neu.f32 	%p76, %f645, 0f7F800000;
	@%p76 bra 	$L__BB0_45;

	setp.ge.f32 	%p77, %f320, 0f00000000;
	selp.b32 	%r338, 2139095040, 0, %p77;
	or.b32  	%r339, %r338, -2147483648;
	selp.b32 	%r340, %r339, %r338, %p3;
	mov.b32 	%f667, %r340;

$L__BB0_45:
	mov.f32 	%f617, 0f3102E308;
	mov.f32 	%f616, 0fBF317218;
	mov.f32 	%f615, 0f3FB8AA3B;
	mov.f32 	%f614, 0f35BFBE8E;
	mov.f32 	%f613, 0f3F317200;
	mov.f32 	%f612, 0f3DAAAABD;
	mov.f32 	%f611, 0f3C4CAF63;
	mov.f32 	%f610, 0f3B18F0FE;
	setp.eq.f32 	%p81, %f320, 0f00000000;
	setp.eq.f32 	%p82, %f319, 0f3F800000;
	or.pred  	%p83, %p82, %p81;
	selp.f32 	%f408, 0f3F800000, %f667, %p83;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f408;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f409, %rs67;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f410, %rs71;}

	// end inline asm
	abs.f32 	%f60, %f409;
	setp.lt.f32 	%p84, %f60, 0f00800000;
	mul.f32 	%f417, %f60, 0f4B800000;
	selp.f32 	%f418, %f417, %f60, %p84;
	selp.f32 	%f419, 0fC3170000, 0fC2FE0000, %p84;
	mov.b32 	%r344, %f418;
	and.b32  	%r345, %r344, 8388607;
	or.b32  	%r346, %r345, 1065353216;
	mov.b32 	%f420, %r346;
	shr.u32 	%r347, %r344, 23;
	cvt.rn.f32.u32 	%f421, %r347;
	add.f32 	%f422, %f419, %f421;
	setp.gt.f32 	%p85, %f420, 0f3FB504F3;
	mul.f32 	%f423, %f420, 0f3F000000;
	add.f32 	%f424, %f422, 0f3F800000;
	selp.f32 	%f425, %f424, %f422, %p85;
	selp.f32 	%f426, %f423, %f420, %p85;
	add.f32 	%f427, %f426, 0fBF800000;
	add.f32 	%f428, %f426, 0f3F800000;
	rcp.approx.ftz.f32 	%f429, %f428;
	add.f32 	%f430, %f427, %f427;
	mul.f32 	%f431, %f430, %f429;
	mul.f32 	%f432, %f431, %f431;
	fma.rn.f32 	%f435, %f610, %f432, %f611;
	fma.rn.f32 	%f437, %f435, %f432, %f612;
	mul.rn.f32 	%f438, %f437, %f432;
	mul.rn.f32 	%f439, %f438, %f431;
	sub.f32 	%f440, %f427, %f431;
	add.f32 	%f441, %f440, %f440;
	neg.f32 	%f442, %f431;
	fma.rn.f32 	%f443, %f442, %f427, %f441;
	mul.rn.f32 	%f444, %f429, %f443;
	add.f32 	%f445, %f439, %f431;
	sub.f32 	%f446, %f431, %f445;
	add.f32 	%f447, %f439, %f446;
	add.f32 	%f448, %f444, %f447;
	add.f32 	%f449, %f445, %f448;
	sub.f32 	%f450, %f445, %f449;
	add.f32 	%f451, %f448, %f450;
	mul.rn.f32 	%f453, %f425, %f613;
	mul.rn.f32 	%f455, %f425, %f614;
	add.f32 	%f456, %f453, %f449;
	sub.f32 	%f457, %f453, %f456;
	add.f32 	%f458, %f449, %f457;
	add.f32 	%f459, %f451, %f458;
	add.f32 	%f460, %f455, %f459;
	add.f32 	%f461, %f456, %f460;
	sub.f32 	%f462, %f456, %f461;
	add.f32 	%f463, %f460, %f462;
	abs.f32 	%f61, %f410;
	setp.gt.f32 	%p86, %f61, 0f77F684DF;
	mul.f32 	%f464, %f410, 0f39000000;
	selp.f32 	%f465, %f464, %f410, %p86;
	mul.rn.f32 	%f466, %f465, %f461;
	neg.f32 	%f467, %f466;
	fma.rn.f32 	%f468, %f465, %f461, %f467;
	fma.rn.f32 	%f469, %f465, %f463, %f468;
	fma.rn.f32 	%f470, %f94, %f461, %f469;
	add.rn.f32 	%f471, %f466, %f470;
	neg.f32 	%f472, %f471;
	add.rn.f32 	%f473, %f466, %f472;
	add.rn.f32 	%f474, %f473, %f470;
	mov.b32 	%r348, %f471;
	setp.eq.s32 	%p87, %r348, 1118925336;
	add.s32 	%r349, %r348, -1;
	mov.b32 	%f475, %r349;
	add.f32 	%f476, %f474, 0f37000000;
	selp.f32 	%f62, %f476, %f474, %p87;
	selp.f32 	%f477, %f475, %f471, %p87;
	mul.rn.f32 	%f479, %f477, %f615;
	cvt.rzi.f32.f32 	%f480, %f479;
	abs.f32 	%f481, %f480;
	setp.gt.f32 	%p88, %f481, 0f42FC0000;
	mov.b32 	%r350, %f480;
	and.b32  	%r351, %r350, -2147483648;
	or.b32  	%r352, %r351, 1123811328;
	mov.b32 	%f482, %r352;
	selp.f32 	%f483, %f482, %f480, %p88;
	fma.rn.f32 	%f485, %f483, %f616, %f477;
	fma.rn.f32 	%f487, %f483, %f617, %f485;
	mul.f32 	%f488, %f487, 0f3FB8AA3B;
	add.f32 	%f489, %f483, 0f4B40007F;
	mov.b32 	%r353, %f489;
	shl.b32 	%r354, %r353, 23;
	mov.b32 	%f490, %r354;
	ex2.approx.ftz.f32 	%f491, %f488;
	mul.f32 	%f63, %f491, %f490;
	setp.eq.f32 	%p89, %f63, 0f7F800000;
	mov.f32 	%f668, 0f7F800000;
	@%p89 bra 	$L__BB0_47;

	fma.rn.f32 	%f668, %f63, %f62, %f63;

$L__BB0_47:
	mul.f32 	%f651, %f410, 0f3F000000;
	cvt.rzi.f32.f32 	%f650, %f651;
	add.f32 	%f649, %f650, %f650;
	sub.f32 	%f648, %f410, %f649;
	abs.f32 	%f647, %f648;
	setp.lt.f32 	%p90, %f409, 0f00000000;
	setp.eq.f32 	%p91, %f647, 0f3F800000;
	and.pred  	%p4, %p90, %p91;
	setp.eq.f32 	%p92, %f409, 0f00000000;
	@%p92 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_48;

$L__BB0_51:
	add.f32 	%f495, %f409, %f409;
	mov.b32 	%r357, %f495;
	selp.b32 	%r358, %r357, 0, %p91;
	or.b32  	%r359, %r358, 2139095040;
	setp.lt.f32 	%p96, %f410, 0f00000000;
	selp.b32 	%r360, %r359, %r358, %p96;
	mov.b32 	%f670, %r360;
	bra.uni 	$L__BB0_52;

$L__BB0_48:
	mov.b32 	%r355, %f668;
	xor.b32  	%r356, %r355, -2147483648;
	mov.b32 	%f492, %r356;
	selp.f32 	%f670, %f492, %f668, %p4;
	setp.geu.f32 	%p93, %f409, 0f00000000;
	@%p93 bra 	$L__BB0_52;

	cvt.rzi.f32.f32 	%f493, %f410;
	setp.eq.f32 	%p94, %f493, %f410;
	@%p94 bra 	$L__BB0_52;

	mov.f32 	%f670, 0f7FFFFFFF;

$L__BB0_52:
	abs.f32 	%f653, %f410;
	abs.f32 	%f652, %f409;
	add.f32 	%f496, %f652, %f653;
	mov.b32 	%r361, %f496;
	setp.lt.s32 	%p97, %r361, 2139095040;
	@%p97 bra 	$L__BB0_59;

	abs.f32 	%f655, %f410;
	abs.f32 	%f654, %f409;
	setp.gtu.f32 	%p98, %f654, 0f7F800000;
	setp.gtu.f32 	%p99, %f655, 0f7F800000;
	or.pred  	%p100, %p98, %p99;
	@%p100 bra 	$L__BB0_58;
	bra.uni 	$L__BB0_54;

$L__BB0_58:
	add.f32 	%f670, %f409, %f410;
	bra.uni 	$L__BB0_59;

$L__BB0_54:
	abs.f32 	%f656, %f410;
	setp.eq.f32 	%p101, %f656, 0f7F800000;
	@%p101 bra 	$L__BB0_57;
	bra.uni 	$L__BB0_55;

$L__BB0_57:
	abs.f32 	%f658, %f409;
	setp.gt.f32 	%p104, %f658, 0f3F800000;
	selp.b32 	%r365, 2139095040, 0, %p104;
	xor.b32  	%r366, %r365, 2139095040;
	setp.lt.f32 	%p105, %f410, 0f00000000;
	selp.b32 	%r367, %r366, %r365, %p105;
	mov.b32 	%f497, %r367;
	setp.eq.f32 	%p106, %f409, 0fBF800000;
	selp.f32 	%f670, 0f3F800000, %f497, %p106;
	bra.uni 	$L__BB0_59;

$L__BB0_55:
	abs.f32 	%f657, %f409;
	setp.neu.f32 	%p102, %f657, 0f7F800000;
	@%p102 bra 	$L__BB0_59;

	setp.ge.f32 	%p103, %f410, 0f00000000;
	selp.b32 	%r362, 2139095040, 0, %p103;
	or.b32  	%r363, %r362, -2147483648;
	selp.b32 	%r364, %r363, %r362, %p4;
	mov.b32 	%f670, %r364;

$L__BB0_59:
	setp.eq.f32 	%p107, %f410, 0f00000000;
	setp.eq.f32 	%p108, %f409, 0f3F800000;
	or.pred  	%p109, %p108, %p107;
	selp.f32 	%f498, 0f3F800000, %f670, %p109;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f498;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f499, %rs74;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f500, %rs78;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f501, %rs81;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f502, %rs84;}

	// end inline asm
	fma.rn.f32 	%f503, %f499, 0f3D372713, %f131;
	fma.rn.f32 	%f504, %f500, 0f3D372713, %f132;
	fma.rn.f32 	%f505, %f501, 0f3D372713, %f133;
	fma.rn.f32 	%f506, %f502, 0f3D372713, %f134;
	mul.f32 	%f73, %f503, 0f3F4C422A;
	mul.f32 	%f74, %f504, 0f3F4C422A;
	mul.f32 	%f75, %f505, 0f3F4C422A;
	mul.f32 	%f76, %f506, 0f3F4C422A;
	abs.f32 	%f77, %f73;
	setp.ltu.f32 	%p110, %f77, 0f3F19999A;
	@%p110 bra 	$L__BB0_61;
	bra.uni 	$L__BB0_60;

$L__BB0_61:
	mul.f32 	%f515, %f73, %f73;
	mov.f32 	%f516, 0fBD563CAE;
	mov.f32 	%f517, 0f3C80F082;
	fma.rn.f32 	%f518, %f517, %f515, %f516;
	mov.f32 	%f519, 0f3E085941;
	fma.rn.f32 	%f520, %f518, %f515, %f519;
	mov.f32 	%f521, 0fBEAAA9ED;
	fma.rn.f32 	%f522, %f520, %f515, %f521;
	fma.rn.f32 	%f524, %f522, %f515, %f94;
	fma.rn.f32 	%f671, %f524, %f73, %f73;
	bra.uni 	$L__BB0_62;

$L__BB0_60:
	mul.f32 	%f507, %f77, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f508, %f507;
	add.f32 	%f509, %f508, 0f3F800000;
	mov.f32 	%f510, 0f3F800000;
	rcp.approx.ftz.f32 	%f511, %f509;
	mov.f32 	%f512, 0fC0000000;
	fma.rn.f32 	%f513, %f511, %f512, %f510;
	setp.ge.f32 	%p111, %f77, 0f41102CB4;
	selp.f32 	%f514, 0f3F800000, %f513, %p111;
	mov.b32 	%r368, %f514;
	mov.b32 	%r369, %f73;
	and.b32  	%r370, %r369, -2147483648;
	or.b32  	%r371, %r370, %r368;
	mov.b32 	%f671, %r371;

$L__BB0_62:
	abs.f32 	%f81, %f74;
	setp.ltu.f32 	%p112, %f81, 0f3F19999A;
	@%p112 bra 	$L__BB0_64;
	bra.uni 	$L__BB0_63;

$L__BB0_64:
	mul.f32 	%f533, %f74, %f74;
	mov.f32 	%f534, 0fBD563CAE;
	mov.f32 	%f535, 0f3C80F082;
	fma.rn.f32 	%f536, %f535, %f533, %f534;
	mov.f32 	%f537, 0f3E085941;
	fma.rn.f32 	%f538, %f536, %f533, %f537;
	mov.f32 	%f539, 0fBEAAA9ED;
	fma.rn.f32 	%f540, %f538, %f533, %f539;
	fma.rn.f32 	%f542, %f540, %f533, %f94;
	fma.rn.f32 	%f672, %f542, %f74, %f74;
	bra.uni 	$L__BB0_65;

$L__BB0_63:
	mul.f32 	%f525, %f81, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f526, %f525;
	add.f32 	%f527, %f526, 0f3F800000;
	mov.f32 	%f528, 0f3F800000;
	rcp.approx.ftz.f32 	%f529, %f527;
	mov.f32 	%f530, 0fC0000000;
	fma.rn.f32 	%f531, %f529, %f530, %f528;
	setp.ge.f32 	%p113, %f81, 0f41102CB4;
	selp.f32 	%f532, 0f3F800000, %f531, %p113;
	mov.b32 	%r372, %f532;
	mov.b32 	%r373, %f74;
	and.b32  	%r374, %r373, -2147483648;
	or.b32  	%r375, %r374, %r372;
	mov.b32 	%f672, %r375;

$L__BB0_65:
	abs.f32 	%f85, %f75;
	setp.ltu.f32 	%p114, %f85, 0f3F19999A;
	@%p114 bra 	$L__BB0_67;
	bra.uni 	$L__BB0_66;

$L__BB0_67:
	mul.f32 	%f551, %f75, %f75;
	mov.f32 	%f552, 0fBD563CAE;
	mov.f32 	%f553, 0f3C80F082;
	fma.rn.f32 	%f554, %f553, %f551, %f552;
	mov.f32 	%f555, 0f3E085941;
	fma.rn.f32 	%f556, %f554, %f551, %f555;
	mov.f32 	%f557, 0fBEAAA9ED;
	fma.rn.f32 	%f558, %f556, %f551, %f557;
	fma.rn.f32 	%f560, %f558, %f551, %f94;
	fma.rn.f32 	%f673, %f560, %f75, %f75;
	bra.uni 	$L__BB0_68;

$L__BB0_66:
	mul.f32 	%f543, %f85, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f544, %f543;
	add.f32 	%f545, %f544, 0f3F800000;
	mov.f32 	%f546, 0f3F800000;
	rcp.approx.ftz.f32 	%f547, %f545;
	mov.f32 	%f548, 0fC0000000;
	fma.rn.f32 	%f549, %f547, %f548, %f546;
	setp.ge.f32 	%p115, %f85, 0f41102CB4;
	selp.f32 	%f550, 0f3F800000, %f549, %p115;
	mov.b32 	%r376, %f550;
	mov.b32 	%r377, %f75;
	and.b32  	%r378, %r377, -2147483648;
	or.b32  	%r379, %r378, %r376;
	mov.b32 	%f673, %r379;

$L__BB0_68:
	abs.f32 	%f89, %f76;
	setp.ltu.f32 	%p116, %f89, 0f3F19999A;
	@%p116 bra 	$L__BB0_70;
	bra.uni 	$L__BB0_69;

$L__BB0_70:
	mul.f32 	%f569, %f76, %f76;
	mov.f32 	%f570, 0fBD563CAE;
	mov.f32 	%f571, 0f3C80F082;
	fma.rn.f32 	%f572, %f571, %f569, %f570;
	mov.f32 	%f573, 0f3E085941;
	fma.rn.f32 	%f574, %f572, %f569, %f573;
	mov.f32 	%f575, 0fBEAAA9ED;
	fma.rn.f32 	%f576, %f574, %f569, %f575;
	fma.rn.f32 	%f578, %f576, %f569, %f94;
	fma.rn.f32 	%f674, %f578, %f76, %f76;
	bra.uni 	$L__BB0_71;

$L__BB0_69:
	mul.f32 	%f561, %f89, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f562, %f561;
	add.f32 	%f563, %f562, 0f3F800000;
	mov.f32 	%f564, 0f3F800000;
	rcp.approx.ftz.f32 	%f565, %f563;
	mov.f32 	%f566, 0fC0000000;
	fma.rn.f32 	%f567, %f565, %f566, %f564;
	setp.ge.f32 	%p117, %f89, 0f41102CB4;
	selp.f32 	%f568, 0f3F800000, %f567, %p117;
	mov.b32 	%r380, %f568;
	mov.b32 	%r381, %f76;
	and.b32  	%r382, %r381, -2147483648;
	or.b32  	%r383, %r382, %r380;
	mov.b32 	%f674, %r383;

$L__BB0_71:
	add.f32 	%f583, %f671, 0f3F800000;
	mul.f32 	%f579, %f1, %f583;
	add.f32 	%f584, %f672, 0f3F800000;
	mul.f32 	%f580, %f2, %f584;
	add.f32 	%f585, %f673, 0f3F800000;
	mul.f32 	%f581, %f3, %f585;
	add.f32 	%f586, %f674, 0f3F800000;
	mul.f32 	%f582, %f4, %f586;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs92, %f582;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs91, %f581;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f580;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f579;}

	// end inline asm
	st.global.v4.u16 	[%rd45], {%rs89, %rs90, %rs91, %rs92};
	add.s64 	%rd45, %rd45, 8192;
	add.s32 	%r400, %r400, 2176;
	add.s32 	%r401, %r401, 1;
	setp.ne.s32 	%p118, %r401, 4;
	@%p118 bra 	$L__BB0_3;

	ret;

}

